Absolutely! Here's your updated and comprehensive **Project Overview & Plan**, tailored to your specific dataset and requirements for the AaltoES 2025 Computer Vision Hackathon.

---

## ğŸ§  Project Overview: AI Manipulation Detection via Segmentation

### ğŸ¯ Goal:
Detect and segment regions in images that have been manipulated by AI using a deep learning segmentation model. The model outputs a **binary mask**, where each pixel is classified as:
- `0` = Not manipulated (real)
- `1` = AI-generated or edited (fake)

### ğŸ“ˆ Output:
- A CSV file with each row containing:
  - `image_id`
  - `predicted mask` in **RLE (Run-Length Encoding)** format

---

## ğŸ“ Updated Dataset Structure

```
dataset/
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/            # 15,000 manipulated images
â”‚   â”œâ”€â”€ masks/             # 15,000 binary masks (white = AI-touched, black = untouched)
â”‚
â”œâ”€â”€ test/
â”‚   â””â”€â”€ images/            # 18,735 manipulated test images (no ground truth)
```

- **Image resolution**: 256x256
- **Mask format**: White (pixel value 255) = manipulated region = `1`, Black (0) = unedited = `0`

---

## ğŸ”§ Project Structure

```
ai-manipulation-detector/
â”‚
â”œâ”€â”€ data/                          # Dataset links or symbolic links
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                  # Training pipeline
â”‚   â”œâ”€â”€ infer.py                 # Inference & RLE encoding
â”‚   â”œâ”€â”€ utils.py                 # Helper functions (RLE encoder, mask postprocessing)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_weights.pth         # Saved model weights
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ predicted_masks/         # Binary mask outputs from inference
â”‚   â””â”€â”€ submission.csv           # Final CSV for submission
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ—ï¸ Model Recommendation

Since manipulated areas can be **circles, squares, or abstract forms**, you need a **general-purpose segmentation model** with high spatial resolution. Here are top choices:

| Model        | Notes                                                                 |
|--------------|----------------------------------------------------------------------|
| **U-Net**    | Lightweight, fast to train, easy to implement. Good baseline.        |
| **DeepLabV3+** | Strong performance, better edge segmentation, but heavier model.    |
| **SegFormer (optional)** | New transformer-based model with strong generalization. More complex. |

ğŸ“Œ **Start with U-Net**, then optionally try DeepLabV3+ later if time allows.

---

## ğŸ§ª Loss Function

Use a **combo of losses** for better performance:
- **Binary Cross Entropy (BCE)** + **Dice Loss**

This combination helps the model focus both on pixel-wise accuracy and region overlap.

---

## ğŸ“Š Evaluation Metric

- **Dice Coefficient** (provided by competition)
- You can also monitor **IoU (Intersection over Union)** during training as a helper metric

---

## ğŸ“¦ Submission Format (RLE)

- Required format:  
```csv
image_id, rle_mask
00001.png, 4 7 100 10 200 8 ...
00002.png, 1 5 90 12 ...
```

- You need to encode each predicted mask using **Run-Length Encoding (RLE)** and output the CSV.

---

## ğŸš€ Full Project Pipeline Summary

### 1. **Training Phase**
- Load images + masks
- Train segmentation model (U-Net or similar)
- Monitor Dice loss (optional: save best model checkpoints)

### 2. **Inference Phase**
- Load trained model
- Predict masks for all `test/images/`
- Postprocess masks (e.g., thresholding if output is probabilistic)
- Convert masks â†’ RLE format
- Write `submission.csv`

---

## âœ… Optional Improvements (if time allows)
- Use TTA (Test Time Augmentation)
- Use image augmentations (rotation, flip, noise) during training
- Try different models (e.g., DeepLabV3+, SegFormer)
- Ensembling (combine multiple model predictions)

---

Would you like me to now help you define a **specific training plan for U-Net**, or give you a short high-level **to-do checklist** so you can proceed smoothly?